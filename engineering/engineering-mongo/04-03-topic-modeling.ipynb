{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from mongo_aggregation_verbs import *\n",
    "\n",
    "from lib import create_mongo_client_to_database_collection\n",
    "\n",
    "collection_reference = create_mongo_client_to_database_collection('twitter', 'tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html\n",
    "- https://alexisperrier.com/nlp/2015/09/04/topic-modeling-of-twitter-followers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_empty_url_arrays = { MATCH : { \"entities.urls\" : [] } }\n",
    "\n",
    "list(collection_reference.aggregate(\n",
    "    [\n",
    "        match_empty_url_arrays,\n",
    "        { COUNT : \"text\" }\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_hashtags = ['job', 'jobs', 'hiring', 'careerarc']\n",
    "location_hashtags = ['california', 'losangeles', 'la', 'santamonica', 'glendale', 'paloalto']\n",
    "match_not_in_bad = { MATCH : { \"text\" : { \"$in\" : job_hashtags + location_hashtags } } }\n",
    "project_to_text_keep_id = { PROJECT : { \"text\" : \"$entities.hashtags.text\" } }\n",
    "project_to_id = { PROJECT : { \"_id\" : 1 } }\n",
    "\n",
    "bad_ids = list(collection_reference.aggregate(\n",
    "    [\n",
    "        match_non_empty_hashtag_arrays,\n",
    "        project_to_text_keep_id,\n",
    "        unwind_text,\n",
    "        project_to_lower,\n",
    "        match_not_in_bad,\n",
    "        project_to_id\n",
    "    ]\n",
    "))\n",
    "bad_ids[:10], len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = [bad_id['_id'] for bad_id in bad_ids]\n",
    "bad_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_bad_ids = { \"$nin\" : bad_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_bad_ids_and_no_url = { \n",
    "    \"_id\"           : not_in_bad_ids, \n",
    "    \"entities.urls\" : []\n",
    "}\n",
    "\n",
    "just_the_text = {\n",
    "    \"text\" : 1,\n",
    "    \"_id\"  : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_reference.find_one(\n",
    "    not_in_bad_ids_and_no_url,\n",
    "    just_the_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur  = collection_reference.find(\n",
    "    not_in_bad_ids_and_no_url,\n",
    "    just_the_text\n",
    ")\n",
    "\n",
    "tweets = list(cur)\n",
    "tweet_text = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_text.text = tweet_text.text.str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(tweet_text.text)\n",
    "word_occurence = tfidf.transform(tweet_text.text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()\n",
    "word_sample = random.sample(words, 20)\n",
    "word_occurence_m = pd.DataFrame(word_occurence, columns=words)\n",
    "word_occurence_m[word_sample].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=10, learning_method='batch')\n",
    "lda.fit(word_occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(lda.components_, columns=words).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_topic(lda_df, index, threshold):\n",
    "    return (lda_df[lda_df[index] > threshold][index]\n",
    "            .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 7, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
